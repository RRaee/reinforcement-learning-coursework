{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PPO.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W44hlCzvF1o7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import datetime\n",
        "from collections import namedtuple\n",
        "from collections import deque\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.clip_grad import clip_grad_norm_\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "Box2DEnvName = 'BipedalWalker-v3'\n",
        "SIZEofBATCH = 64\n",
        "EPOCHS_PPO = 5\n",
        "MAXIMUM_ITERATION = 25000\n",
        "DEVICE = 'cpu'\n",
        "GRADIENTCLIP = 0.1\n",
        "SIZE_TRAJECTORY = 2048\n",
        "GAE_LAMBDA = 0.90\n",
        "EPSCLIP = 0.1\n",
        "GAMMA = 0.98"
      ],
      "metadata": {
        "id": "7L2o8-pzF5l8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the Hyperparameters\n",
        "TEST_VIDEO_SAVE = True\n",
        "N_ITER_TEST = 100\n",
        "POLICY_LR = 0.0003\n",
        "VALUE_LR = 0.001\n",
        "EPISODESTESTED = 5\n",
        "BESTRESULTSTESTED = -1e5\n",
        "now = datetime.datetime.now()\n",
        "TIMEDATE = \"{}_{}.{}.{}\".format(now.day, now.hour, now.minute, now.second)\n",
        "MODELLOADED = False\n",
        "NameCheckpoint = \"checkpoints/...\""
      ],
      "metadata": {
        "id": "Bn3f7C1bF5qM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class A2CPOLICY(nn.Module):\n",
        "   \n",
        "    def __init__(self, inputShape, NActions):\n",
        "        super(A2CPOLICY, self).__init__()\n",
        "\n",
        "        self.lp = nn.Sequential(\n",
        "            nn.Linear(inputShape[0], 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 32),\n",
        "            nn.ReLU())\n",
        "\n",
        "        self.mean_l = nn.Linear(32, NActions[0])\n",
        "        self.mean_l.weight.data.mul_(0.1)\n",
        "\n",
        "        self.var_l = nn.Linear(32, NActions[0])\n",
        "        self.var_l.weight.data.mul_(0.1)\n",
        "\n",
        "        self.logstd = nn.Parameter(torch.zeros(NActions[0]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        ot_n = self.lp(x.float())\n",
        "        return F.tanh(self.mean_l(ot_n))"
      ],
      "metadata": {
        "id": "wLiOHLkFF5tt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class A2Cvalue(nn.Module):\n",
        "    \n",
        "    def __init__(self, inputShape):\n",
        "        super(A2Cvalue, self).__init__()\n",
        "\n",
        "        self.lp = nn.Sequential(\n",
        "            nn.Linear(inputShape[0], 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lp(x.float())"
      ],
      "metadata": {
        "id": "mtBuwV03F5wY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Environment:\n",
        "  \n",
        "    gameReward = 0\n",
        "    lastGameReward = 0\n",
        "    gameN = 0\n",
        "    lastGamesRews = [-200]\n",
        "    n_iter = 0\n",
        "\n",
        "    def __init__(self, Box2DEnvName, Nsteps, GAMMA, GAE_LAMBDA, save_video=False):\n",
        "        super(Environment, self).__init__()\n",
        "\n",
        "        # create the new environment\n",
        "        self.Environment = gym.make(Box2DEnvName)\n",
        "        self.observation = self.Environment.reset()\n",
        "\n",
        "        self.Nsteps = Nsteps\n",
        "        self.Naction = self.Environment.action_space.shape\n",
        "        self.Nobservation = self.env.observation_space.shape[0]\n",
        "        self.GAMMA = GAMMA\n",
        "        self.GAE_LAMBDA = GAE_LAMBDA"
      ],
      "metadata": {
        "id": "icGT7jvCF5zK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def steps(self, agentPolicy, agentValue):\n",
        "     \n",
        "        memories = []\n",
        "        for s in range(self.Nsteps):\n",
        "            self.n_iter += 1\n",
        "\n",
        "           \n",
        "            agentMean = agent_policy(torch.tensor(self.observation))\n",
        "\n",
        "           \n",
        "            logstd = agent_policy.logstd.data.cpu().numpy()\n",
        "            Action = ag_mean.data.cpu().numpy() + np.exp(logstd) * np.random.normal(size=logstd.shape)\n",
        "            Action = np.clip(action, -1, 1)\n",
        "\n",
        "            stateValue = float(agentValue(torch.tensor(self.observation)))\n",
        "\n",
        "       \n",
        "            new_observation, reward, done, _ = self.Environment.step(Action)\n",
        "\n",
        "        \n",
        "            if done:\n",
        "            \n",
        "                memories.append(Memory(observation=self.observation, action=action, new_observation=new_observation, reward=0, done=done, value=state_value, adv=0))\n",
        "            else:\n",
        "                memories.append(Memory(obs=self.observation, Action=Action, new_observation=new_observation, reward=reward, done=done, value=stateValue, adv=0))\n",
        "\n",
        "\n",
        "            self.game_rew += reward\n",
        "            self.obs = new_observation\n",
        "\n",
        "            if done:\n",
        "                print('#####',self.game_n, 'rew:', int(self.game_rew), int(np.mean(self.last_games_rews[-100:])), np.round(reward,2), self.n_iter)\n",
        "\n",
        "               \n",
        "                self.obs = self.env.reset()\n",
        "                self.last_game_rew = self.game_rew\n",
        "                self.game_rew = 0\n",
        "                self.game_n += 1\n",
        "                self.n_iter = 0\n",
        "                self.last_games_rews.append(self.last_game_rew)\n",
        "\n",
        " \n",
        "        return self.generalized_advantage_estimation(memories)\n",
        "\n",
        "def generalized_advantage_estimation(self, memories):\n",
        "        \n",
        "        upd_memories = []\n",
        "        run_add = 0\n",
        "\n",
        "        for t in reversed(range(len(memories)-1)):\n",
        "            if memories[t].done:\n",
        "                run_add = memories[t].reward\n",
        "            else:\n",
        "                sigma = memories[t].reward + self.gamma * memories[t+1].value - memories[t].value\n",
        "                run_add = sigma + run_add * self.gamma * self.gae_lambda\n",
        "\n",
        "           \n",
        "          \n",
        "            upd_memories.append(Memory(obs=memories[t].obs, action=memories[t].action, new_obs=memories[t].new_obs, reward=run_add + memories[t].value, done=memories[t].done, value=memories[t].value, adv=run_add))\n",
        "\n",
        "        return upd_memories[::-1]\n",
        "\n",
        "\n",
        "def log_policy_prob(mean, std, actions):\n",
        "    \n",
        "    act_log_softmax = -((mean-actions)**2)/(2*torch.exp(std).clamp(min=1e-4)) - torch.log(torch.sqrt(2*math.pi*torch.exp(std)))\n",
        "    return act_log_softmax\n",
        "\n",
        "def compute_log_policy_prob(memories, nn_policy, device):\n",
        "    \n",
        "    n_mean = nn_policy(torch.tensor(np.array([m.obs for m in memories], dtype=np.float32)).to(device))\n",
        "    n_mean = n_mean.type(torch.DoubleTensor)\n",
        "    logstd = agent_policy.logstd.type(torch.DoubleTensor)\n",
        "\n",
        "    actions = torch.DoubleTensor(np.array([m.action for m in memories])).to(device)\n",
        "\n",
        "    return log_policy_prob(n_mean, logstd, actions)\n",
        "\n",
        "def clipped_PPO_loss(memories, nn_policy, nn_value, old_log_policy, adv, epsilon, writer, device):\n",
        "\n",
        "   \n",
        "    rewards = torch.tensor(np.array([m.reward for m in memories], dtype=np.float32)).to(device)\n",
        "    value = nn_value(torch.tensor(np.array([m.obs for m in memories], dtype=np.float32)).to(device))\n",
        "    \n",
        "    # Value loss\n",
        "    vl_loss = F.mse_loss(value.squeeze(-1), rewards)\n",
        "\n",
        "    new_log_policy = compute_log_policy_prob(memories, nn_policy, device)\n",
        "    rt_theta = torch.exp(new_log_policy - old_log_policy.detach())\n",
        "\n",
        "    adv = adv.unsqueeze(-1) # add a dimension because rt_theta has shape: [batch_size, n_actions]\n",
        "    pg_loss = -torch.mean(torch.min(rt_theta*adv, torch.clamp(rt_theta, 1-epsilon, 1+epsilon)*adv))\n",
        "\n",
        "    return pg_loss, vl_loss\n",
        "\n",
        "def test_game(tst_env, agent_policy, test_episodes):\n",
        "   \n",
        "    reward_games = []\n",
        "    steps_games = []\n",
        "    for _ in range(test_episodes):\n",
        "        obs = tst_env.reset()\n",
        "        rewards = 0\n",
        "        steps = 0\n",
        "        while True:\n",
        "            ag_mean = agent_policy(torch.tensor(obs))\n",
        "            action = np.clip(ag_mean.data.cpu().numpy().squeeze(), -1, 1)\n",
        "\n",
        "            next_obs, reward, done, _ = tst_env.step(action)\n",
        "            steps += 1\n",
        "            obs = next_obs\n",
        "            rewards += reward\n",
        "\n",
        "            if done:\n",
        "                reward_games.append(rewards)\n",
        "                steps_games.append(steps)\n",
        "                obs = tst_env.reset()\n",
        "                break\n",
        "\n",
        "    return np.mean(reward_games), np.mean(steps_games)\n",
        "\n",
        "\n",
        "Memory = namedtuple('Memory', ['obs', 'action', 'new_obs', 'reward', 'done', 'value', 'adv'],rename=False)"
      ],
      "metadata": {
        "id": "rg39dnNyF52I"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install box2d-py\n",
        "!pip install gym[all]\n",
        "!pip install gym[Box_2D]\n",
        "!pip install git+https://github.com/AI4Finance-LLC/ElegantRL.git\n",
        "\n",
        "import gym\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  \n",
        "    env = Environment(Box2DEnvName, SIZE_TRAJECTORY, GAMMA, GAE_LAMBDA)\n",
        "\n",
        "    writer_name = 'PPO_'+ENV_NAME+'_'+date_time+'_'+str(POLICY_LR)+'_'+str(VALUE_LR)+'_'+str(TRAJECTORY_SIZE)+'_'+str(BATCH_SIZE)\n",
        "    writer = SummaryWriter(log_dir='content/runs/'+writer_name)\n",
        "\n",
        "   \n",
        "    test_env = gym.make(Box2DEnvName)\n",
        "    if save_video_test:\n",
        "        test_env = gym.wrappers.Monitor(test_env,  \"VIDEOS/TEST_VIDEOS_\"+writer_name, video_callable=lambda episode_id: episode_id%10==0)\n",
        "\n",
        "\n",
        "    agent_policy = A2C_policy(test_env.observation_space.shape, test_env.action_space.shape).to(device)\n",
        "    agent_value = A2C_value(test_env.observation_space.shape).to(device)\n",
        "\n",
        "\n",
        "    optimizer_policy = optim.Adam(agent_policy.parameters(), lr=POLICY_LR)\n",
        "    optimizer_value = optim.Adam(agent_value.parameters(), lr=VALUE_LR)\n",
        "\n",
        "    if load_model:\n",
        "        print('> Loading checkpoint {}'.format(checkpoint_name))\n",
        "        checkpoint = torch.load(checkpoint_name)\n",
        "        agent_policy.load_state_dict(checkpoint['agent_policy'])\n",
        "        agent_value.load_state_dict(checkpoint['agent_value'])\n",
        "        optimizer_policy.load_state_dict(checkpoint['optimizer_policy'])\n",
        "        optimizer_value.load_state_dict(checkpoint['optimizer_value'])\n",
        "\n",
        "\n",
        "    experience = []\n",
        "    n_iter = 0\n",
        "\n",
        "    while n_iter < MAX_ITER:\n",
        "        n_iter += 1\n",
        "\n",
        "        batch = env.steps(agent_policy, agent_value)\n",
        "\n",
        "      \n",
        "        old_log_policy = compute_log_policy_prob(batch, agent_policy, device)\n",
        "\n",
        "\n",
        "        batch_adv = np.array([m.adv for m in batch])\n",
        "        \n",
        "        batch_adv = (batch_adv - np.mean(batch_adv)) / (np.std(batch_adv) + 1e-7)\n",
        "        batch_adv = torch.tensor(batch_adv).to(device)\n",
        "\n",
        "       \n",
        "        pol_loss_acc = []\n",
        "        val_loss_acc = []\n",
        "\n",
        "       \n",
        "        for s in range(PPO_EPOCHS):\n",
        "         \n",
        "            for mb in range(0, len(batch), BATCH_SIZE):\n",
        "                mini_batch = batch[mb:mb+BATCH_SIZE]\n",
        "                minib_old_log_policy = old_log_policy[mb:mb+BATCH_SIZE]\n",
        "                minib_adv = batch_adv[mb:mb+BATCH_SIZE]\n",
        "\n",
        "              \n",
        "                pol_loss, val_loss = clipped_PPO_loss(mini_batch, agent_policy, agent_value, minib_old_log_policy, minib_adv, CLIP_EPS, writer, device)\n",
        "\n",
        "               \n",
        "                optimizer_policy.zero_grad()\n",
        "                pol_loss.backward()\n",
        "                optimizer_policy.step()\n",
        "\n",
        "                \n",
        "                optimizer_value.zero_grad()\n",
        "                val_loss.backward()\n",
        "                optimizer_value.step()\n",
        "\n",
        "                pol_loss_acc.append(float(pol_loss))\n",
        "                val_loss_acc.append(float(val_loss))\n",
        "\n",
        "       \n",
        "        writer.add_scalar('pg_loss', np.mean(pol_loss_acc), n_iter)\n",
        "        writer.add_scalar('vl_loss', np.mean(val_loss_acc), n_iter)\n",
        "        writer.add_scalar('rew', env.last_game_rew, n_iter)\n",
        "        writer.add_scalar('10rew', np.mean(env.last_games_rews[-100:]), n_iter)\n",
        "\n",
        "        \n",
        "        if n_iter % N_ITER_TEST == 0:\n",
        "            test_rews, test_stps = test_game(test_env, agent_policy, test_episodes)\n",
        "            print(' > Testing..', n_iter,test_rews, test_stps)\n",
        "         \n",
        "            if test_rews > best_test_result:\n",
        "                torch.save({\n",
        "                    'agent_policy': agent_policy.state_dict(),\n",
        "                    'agent_value': agent_value.state_dict(),\n",
        "                    'optimizer_policy': optimizer_policy.state_dict(),\n",
        "                    'optimizer_value': optimizer_value.state_dict(),\n",
        "                    'test_reward': test_rews\n",
        "                }, 'checkpoints/checkpoint_'+writer_name+'.pth.tar')\n",
        "                best_test_result = test_rews\n",
        "                print('=> Best test!! Reward:{:.2f}  Steps:{}'.format(test_rews, test_stps))\n",
        "\n",
        "            writer.add_scalar('test_rew', test_rews, n_iter)\n",
        "\n",
        "\n",
        "    writer.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6_Qq0mpxF55N",
        "outputId": "9b9240eb-1be9-4001-c3a5-d81b93661251"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting box2d-py\n",
            "  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 24.2 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 28.4 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30 kB 18.5 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████                        | 112 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 122 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 174 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 194 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 225 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 245 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 296 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 317 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 337 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 348 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 368 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 389 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 399 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 409 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 440 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 448 kB 5.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Requirement already satisfied: gym[all] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[all]) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (1.21.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[all]) (7.1.2)\n",
            "Requirement already satisfied: box2d-py~=2.3.5 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (2.3.8)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym[all]) (4.1.2.30)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from gym[all]) (2.4.1)\n",
            "Collecting mujoco-py<2.0,>=1.50\n",
            "  Downloading mujoco-py-1.50.1.68.tar.gz (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (0.2.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[all]) (1.15.0)\n",
            "Collecting glfw>=1.4.0\n",
            "  Downloading glfw-2.5.3-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (206 kB)\n",
            "\u001b[K     |████████████████████████████████| 206 kB 33.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.7/dist-packages (from mujoco-py<2.0,>=1.50->gym[all]) (0.29.28)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.7/dist-packages (from mujoco-py<2.0,>=1.50->gym[all]) (1.15.0)\n",
            "Collecting lockfile>=0.12.2\n",
            "  Downloading lockfile-0.12.2-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.10->mujoco-py<2.0,>=1.50->gym[all]) (2.21)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[all]) (0.16.0)\n",
            "Building wheels for collected packages: mujoco-py\n",
            "  Building wheel for mujoco-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for mujoco-py\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for mujoco-py\n",
            "Failed to build mujoco-py\n",
            "Installing collected packages: lockfile, glfw, mujoco-py\n",
            "    Running setup.py install for mujoco-py ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-lzqhj3fy/mujoco-py_d05817ba70274bbda7b9555cde35c40a/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-lzqhj3fy/mujoco-py_d05817ba70274bbda7b9555cde35c40a/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-q8h4irov/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.7/mujoco-py Check the logs for full command output.\u001b[0m\n",
            "Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "\u001b[33mWARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.21.6)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n",
            "Collecting git+https://github.com/AI4Finance-LLC/ElegantRL.git\n",
            "  Cloning https://github.com/AI4Finance-LLC/ElegantRL.git to /tmp/pip-req-build-5y0rone_\n",
            "  Running command git clone -q https://github.com/AI4Finance-LLC/ElegantRL.git /tmp/pip-req-build-5y0rone_\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from elegantrl==0.3.3) (0.17.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from elegantrl==0.3.3) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from elegantrl==0.3.3) (1.21.6)\n",
            "Collecting pybullet\n",
            "  Downloading pybullet-3.2.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (91.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 91.7 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from elegantrl==0.3.3) (1.10.0+cu111)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from elegantrl==0.3.3) (4.1.2.30)\n",
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (from elegantrl==0.3.3) (2.3.8)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym->elegantrl==0.3.3) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->elegantrl==0.3.3) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->elegantrl==0.3.3) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->elegantrl==0.3.3) (0.16.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->elegantrl==0.3.3) (3.0.8)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->elegantrl==0.3.3) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->elegantrl==0.3.3) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->elegantrl==0.3.3) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->elegantrl==0.3.3) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->elegantrl==0.3.3) (1.15.0)\n",
            "Building wheels for collected packages: elegantrl\n",
            "  Building wheel for elegantrl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for elegantrl: filename=elegantrl-0.3.3-py3-none-any.whl size=240117 sha256=d4b80f02a26a1ea0e8301e353fff8b6ddc22c132160ab64834d36392e48e1f1a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1eoz6a9i/wheels/52/9a/b3/08c8a0b5be22a65da0132538c05e7e961b1253c90d6845e0c6\n",
            "Successfully built elegantrl\n",
            "Installing collected packages: pybullet, elegantrl\n",
            "Successfully installed elegantrl-0.3.3 pybullet-3.2.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-125cf7dfbfd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Customising the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBox2DEnvName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSIZE_TRAJECTORY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAE_LAMBDA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mwriter_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'PPO_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mENV_NAME\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdate_time\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPOLICY_LR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVALUE_LR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAJECTORY_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-219e340fec6f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, Box2DEnvName, Nsteps, GAMMA, GAE_LAMBDA, save_video)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNsteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNsteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNaction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGAMMA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGAE_LAMBDA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAE_LAMBDA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Environment' object has no attribute 'env'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "F6lV3YWCF57r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kiwx9RlnF5-c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}